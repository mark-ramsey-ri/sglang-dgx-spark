# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# SGLang DGX Spark Cluster Configuration
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Copy this file and customize for your deployment:
#   cp config.env config.local.env
#   vim config.local.env
#   source config.local.env
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Model Configuration                                                     │
# └─────────────────────────────────────────────────────────────────────────┘

# Model to serve (HuggingFace model ID or local path)
# Recommended models for DGX Spark 2-node cluster:
#   - openai/gpt-oss-120b        (GPT-OSS 120B - requires 2 nodes)
#   - openai/gpt-oss-20b         (GPT-OSS 20B - fits on 1 node)
#   - meta-llama/Llama-3.3-70B-Instruct
#   - nvidia/Llama-3.3-70B-Instruct-FP4
#   - Qwen/Qwen2.5-72B-Instruct
MODEL="${MODEL:-openai/gpt-oss-120b}"

# Tensor parallelism size (total GPUs across all nodes)
# For 2x DGX Spark (1 GPU each): TP=2 splits model across both GPUs
# TP=2 achieves ~3.2 tok/s (tested with Llama 3.3 70B)
TENSOR_PARALLEL="${TENSOR_PARALLEL:-2}"

# Pipeline parallelism size (number of pipeline stages)
# PP=1 means no pipeline parallelism (default, recommended for DGX Spark)
# PP>1 enables pipeline stages but is slower (~1.7 tok/s) on DGX Spark
# due to sequential processing and single GPU per node
PIPELINE_PARALLEL="${PIPELINE_PARALLEL:-1}"

# Number of nodes in the cluster
NUM_NODES="${NUM_NODES:-2}"

# Memory fraction for KV cache (reduce if OOM)
MEM_FRACTION="${MEM_FRACTION:-0.90}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Docker Configuration                                                    │
# └─────────────────────────────────────────────────────────────────────────┘

# SGLang Docker image
# Use lmsysorg/sglang:spark for DGX Spark optimized builds
# Use lmsysorg/sglang:latest for general deployments
SGLANG_IMAGE="${SGLANG_IMAGE:-lmsysorg/sglang:spark}"

# Container names
HEAD_CONTAINER_NAME="${HEAD_CONTAINER_NAME:-sglang-head}"
WORKER_CONTAINER_NAME="${WORKER_CONTAINER_NAME:-sglang-worker}"

# Shared memory size for Docker containers
SHM_SIZE="${SHM_SIZE:-32g}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Worker Node Configuration (for multi-node clusters)                     │
# └─────────────────────────────────────────────────────────────────────────┘

# WORKER_HOST: Ethernet IP for SSH access to worker node
# Example: 192.168.7.111
WORKER_HOST="${WORKER_HOST:-}"

# WORKER_IB_IP: InfiniBand IP for NCCL communication
# Run 'ibdev2netdev && ip addr show <interface>' on worker to find it
# Example: 169.254.216.8
WORKER_IB_IP="${WORKER_IB_IP:-}"

# Username for SSH to worker nodes (default: current user)
WORKER_USER="${WORKER_USER:-$(whoami)}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Network Configuration                                                   │
# └─────────────────────────────────────────────────────────────────────────┘

# Port for SGLang HTTP API
SGLANG_PORT="${SGLANG_PORT:-30000}"

# Port for distributed initialization (inter-node communication)
DIST_INIT_PORT="${DIST_INIT_PORT:-50000}"

# HEAD_IP: InfiniBand IP of the head node
# Leave empty for auto-detection (recommended)
HEAD_IP="${HEAD_IP:-}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Storage Configuration                                                   │
# └─────────────────────────────────────────────────────────────────────────┘

# HuggingFace cache directory on host
# Models are stored here and shared via bind mount
HF_CACHE="${HF_CACHE:-/raid/hf-cache}"

# Tiktoken encodings directory (for GPT-OSS models)
TIKTOKEN_DIR="${TIKTOKEN_DIR:-${HOME}/tiktoken_encodings}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ Authentication                                                          │
# └─────────────────────────────────────────────────────────────────────────┘

# HuggingFace token for gated models (e.g., Llama)
# Set via: export HF_TOKEN=hf_xxx
HF_TOKEN="${HF_TOKEN:-}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ SGLang Server Options                                                   │
# └─────────────────────────────────────────────────────────────────────────┘

# Reasoning parser (for GPT-OSS models)
# Options: gpt-oss, none
REASONING_PARSER="${REASONING_PARSER:-gpt-oss}"

# Tool call parser (for GPT-OSS models)
TOOL_CALL_PARSER="${TOOL_CALL_PARSER:-gpt-oss}"

# Disable CUDA graph
# Multi-node Tensor Parallel (TP across nodes) requires CUDA graphs to be disabled
# due to IPC limitations. Set to "true" (default for multi-node TP).
# Only set to "false" if using single-node or Pipeline Parallel mode.
DISABLE_CUDA_GRAPH="${DISABLE_CUDA_GRAPH:-true}"

# Additional SGLang arguments (advanced users)
# Example: EXTRA_ARGS="--max-total-tokens 8192 --enable-prefix-caching"
# NOTE: --enable-dp-attention is used to bypass FlashInfer AllReduce Fusion
# which uses CUDA IPC that doesn't work across nodes. This may have performance implications.
EXTRA_ARGS="${EXTRA_ARGS:---enable-dp-attention}"

# ┌─────────────────────────────────────────────────────────────────────────┐
# │ NCCL / InfiniBand Configuration                                         │
# └─────────────────────────────────────────────────────────────────────────┘

# NCCL debug level (INFO, WARN, TRACE)
NCCL_DEBUG="${NCCL_DEBUG:-INFO}"

# InfiniBand HCA devices (auto-detected if empty)
# For DGX Spark, typically: rocep1s0f1,roceP2p1s0f1
# Leave empty for auto-detection
NCCL_IB_HCA="${NCCL_IB_HCA:-}"

# Network interface for NCCL/GLOO (auto-detected if empty)
# For DGX Spark, typically: enp1s0f1np1
NCCL_SOCKET_IFNAME="${NCCL_SOCKET_IFNAME:-}"
GLOO_SOCKET_IFNAME="${GLOO_SOCKET_IFNAME:-}"

# Disable InfiniBand (set to 1 if using Ethernet only)
NCCL_IB_DISABLE="${NCCL_IB_DISABLE:-0}"

# GPUDirect RDMA level (0-5, higher = more aggressive)
NCCL_NET_GDR_LEVEL="${NCCL_NET_GDR_LEVEL:-5}"

# NCCL timeout in milliseconds (default 5 min = 300000ms)
# Increased to 20 min for models that need longer distributed initialization
# (e.g., Qwen2.5-32B, Mixtral-8x7B, Mistral-Nemo-12B)
NCCL_TIMEOUT="${NCCL_TIMEOUT:-1200000}"
